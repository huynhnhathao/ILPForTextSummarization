{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sumo1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "add62610e9cc4ebeb8948a6e4bfe2811": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1d351bb9963546eea1c4a9037a88bf3d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_13352c2ea9c040d8a8a1c204fb9e0ef5",
              "IPY_MODEL_e09d4d17f1b44a5a99717522df7da93e"
            ]
          }
        },
        "1d351bb9963546eea1c4a9037a88bf3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "13352c2ea9c040d8a8a1c204fb9e0ef5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a099ca16de634596b1de973c766835f8",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 434,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 434,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_94c300253f324f6b9ac0545503c40bca"
          }
        },
        "e09d4d17f1b44a5a99717522df7da93e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6857bedeebde418da06c0de31f802a82",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 434/434 [00:00&lt;00:00, 2.43kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d2c8d793d6594f3e8d2af985a2bfb76f"
          }
        },
        "a099ca16de634596b1de973c766835f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "94c300253f324f6b9ac0545503c40bca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6857bedeebde418da06c0de31f802a82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d2c8d793d6594f3e8d2af985a2bfb76f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a7161ba935ac4aebac9b46e0519a8e32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8226e1a83e4d43359acc1b6c0e54b033",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d245b87566114d429e8501dd882ce479",
              "IPY_MODEL_55e1dcb4088940fe9f3b59845ef702e5"
            ]
          }
        },
        "8226e1a83e4d43359acc1b6c0e54b033": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d245b87566114d429e8501dd882ce479": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_360bc5ba3fb449dfa63b8ae24060c7a3",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1344997306,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1344997306,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8bb06d27851247da919deb208657e731"
          }
        },
        "55e1dcb4088940fe9f3b59845ef702e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_bb5e553f7f2f433da24f175e60062cc3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.34G/1.34G [00:19&lt;00:00, 70.3MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f277b1e1f8094e5592e45d0f3e9be0a5"
          }
        },
        "360bc5ba3fb449dfa63b8ae24060c7a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8bb06d27851247da919deb208657e731": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bb5e553f7f2f433da24f175e60062cc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f277b1e1f8094e5592e45d0f3e9be0a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6f80d0e4f1f846299a30418bb580b724": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_14b56cc9ae6c49e48ac639df1594a4b2",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ce96f39439424a1793b6d7b48877f947",
              "IPY_MODEL_38fdfa26c34f496c944f440b66ffa5ee"
            ]
          }
        },
        "14b56cc9ae6c49e48ac639df1594a4b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ce96f39439424a1793b6d7b48877f947": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_068d8478fc6a4849a0ee057672786737",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_209ad5b11a00494ab10c123405ce356c"
          }
        },
        "38fdfa26c34f496c944f440b66ffa5ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_fd36e56966b44e22ba6c4dd274e2261c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 3.13MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_653c2e1c7b6a409080f4327a731d1881"
          }
        },
        "068d8478fc6a4849a0ee057672786737": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "209ad5b11a00494ab10c123405ce356c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fd36e56966b44e22ba6c4dd274e2261c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "653c2e1c7b6a409080f4327a731d1881": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ha8-MzuKlyEj",
        "outputId": "c14c704a-4320-4bea-a981-8d39c8569042"
      },
      "source": [
        "!pip install pulp\r\n",
        "!sudo apt install libglpk-dev python3.8-dev libgmp3-dev\r\n",
        "!pip install glpk\r\n",
        "!pip install rouge-score\r\n",
        "\r\n",
        "import pulp\r\n",
        "import nltk\r\n",
        "import numpy as np\r\n",
        "from rouge_score import rouge_scorer\r\n",
        "import warnings\r\n",
        "warnings.filterwarnings(\"ignore\")\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pulp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/c4/0eec14a0123209c261de6ff154ef3be5cad3fd557c084f468356662e0585/PuLP-2.4-py3-none-any.whl (40.6MB)\n",
            "\u001b[K     |████████████████████████████████| 40.6MB 79kB/s \n",
            "\u001b[?25hCollecting amply>=0.1.2\n",
            "  Downloading https://files.pythonhosted.org/packages/f3/c5/dfa09dd2595a2ab2ab4e6fa7bebef9565812722e1980d04b0edce5032066/amply-0.1.4-py3-none-any.whl\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from amply>=0.1.2->pulp) (2.4.7)\n",
            "Requirement already satisfied: docutils>=0.3 in /usr/local/lib/python3.6/dist-packages (from amply>=0.1.2->pulp) (0.16)\n",
            "Installing collected packages: amply, pulp\n",
            "Successfully installed amply-0.1.4 pulp-2.4\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libamd2 libbtf1 libcamd2 libccolamd2 libcholmod3 libcolamd2 libcxsparse3\n",
            "  libglpk40 libgmp-dev libgmpxx4ldbl libgraphblas1 libklu1 libldl2 libmetis5\n",
            "  libpython3.8 libpython3.8-dev libpython3.8-minimal libpython3.8-stdlib\n",
            "  librbio2 libspqr2 libsuitesparse-dev libsuitesparseconfig5 libumfpack5\n",
            "  python3.8 python3.8-minimal\n",
            "Suggested packages:\n",
            "  libiodbc2-dev default-libmysqlclient-dev gmp-doc libgmp10-doc libmpfr-dev\n",
            "  python3.8-venv python3.8-doc binfmt-support\n",
            "The following NEW packages will be installed:\n",
            "  libamd2 libbtf1 libcamd2 libccolamd2 libcholmod3 libcolamd2 libcxsparse3\n",
            "  libglpk-dev libglpk40 libgmp-dev libgmp3-dev libgmpxx4ldbl libgraphblas1\n",
            "  libklu1 libldl2 libmetis5 libpython3.8 libpython3.8-dev libpython3.8-minimal\n",
            "  libpython3.8-stdlib librbio2 libspqr2 libsuitesparse-dev\n",
            "  libsuitesparseconfig5 libumfpack5 python3.8 python3.8-dev python3.8-minimal\n",
            "0 upgraded, 28 newly installed, 0 to remove and 16 not upgraded.\n",
            "Need to get 64.8 MB of archives.\n",
            "After this operation, 140 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 libpython3.8-minimal amd64 3.8.0-3~18.04 [704 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 python3.8-minimal amd64 3.8.0-3~18.04 [1,816 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsuitesparseconfig5 amd64 1:5.1.2-2 [9,044 B]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 libamd2 amd64 1:5.1.2-2 [19.5 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 libbtf1 amd64 1:5.1.2-2 [10.6 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 libcamd2 amd64 1:5.1.2-2 [20.9 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 libccolamd2 amd64 1:5.1.2-2 [21.7 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 libcolamd2 amd64 1:5.1.2-2 [16.2 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmetis5 amd64 5.1.0.dfsg-5 [169 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 libcholmod3 amd64 1:5.1.2-2 [300 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic/main amd64 libcxsparse3 amd64 1:5.1.2-2 [63.9 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libglpk40 amd64 4.65-1 [378 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgmpxx4ldbl amd64 2:6.1.2+dfsg-2 [8,964 B]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgmp-dev amd64 2:6.1.2+dfsg-2 [316 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgraphblas1 amd64 1:5.1.2-2 [384 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic/main amd64 libklu1 amd64 1:5.1.2-2 [69.3 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic/main amd64 libldl2 amd64 1:5.1.2-2 [10.3 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic/main amd64 libumfpack5 amd64 1:5.1.2-2 [229 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic/main amd64 librbio2 amd64 1:5.1.2-2 [24.0 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic/main amd64 libspqr2 amd64 1:5.1.2-2 [64.2 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsuitesparse-dev amd64 1:5.1.2-2 [1,235 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libglpk-dev amd64 4.65-1 [445 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgmp3-dev amd64 2:6.1.2+dfsg-2 [1,996 B]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 libpython3.8-stdlib amd64 3.8.0-3~18.04 [1,677 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 libpython3.8 amd64 3.8.0-3~18.04 [1,630 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 libpython3.8-dev amd64 3.8.0-3~18.04 [54.3 MB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 python3.8 amd64 3.8.0-3~18.04 [355 kB]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 python3.8-dev amd64 3.8.0-3~18.04 [510 kB]\n",
            "Fetched 64.8 MB in 3s (23.7 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 28.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libpython3.8-minimal:amd64.\n",
            "(Reading database ... 145483 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libpython3.8-minimal_3.8.0-3~18.04_amd64.deb ...\n",
            "Unpacking libpython3.8-minimal:amd64 (3.8.0-3~18.04) ...\n",
            "Selecting previously unselected package python3.8-minimal.\n",
            "Preparing to unpack .../01-python3.8-minimal_3.8.0-3~18.04_amd64.deb ...\n",
            "Unpacking python3.8-minimal (3.8.0-3~18.04) ...\n",
            "Selecting previously unselected package libsuitesparseconfig5:amd64.\n",
            "Preparing to unpack .../02-libsuitesparseconfig5_1%3a5.1.2-2_amd64.deb ...\n",
            "Unpacking libsuitesparseconfig5:amd64 (1:5.1.2-2) ...\n",
            "Selecting previously unselected package libamd2:amd64.\n",
            "Preparing to unpack .../03-libamd2_1%3a5.1.2-2_amd64.deb ...\n",
            "Unpacking libamd2:amd64 (1:5.1.2-2) ...\n",
            "Selecting previously unselected package libbtf1:amd64.\n",
            "Preparing to unpack .../04-libbtf1_1%3a5.1.2-2_amd64.deb ...\n",
            "Unpacking libbtf1:amd64 (1:5.1.2-2) ...\n",
            "Selecting previously unselected package libcamd2:amd64.\n",
            "Preparing to unpack .../05-libcamd2_1%3a5.1.2-2_amd64.deb ...\n",
            "Unpacking libcamd2:amd64 (1:5.1.2-2) ...\n",
            "Selecting previously unselected package libccolamd2:amd64.\n",
            "Preparing to unpack .../06-libccolamd2_1%3a5.1.2-2_amd64.deb ...\n",
            "Unpacking libccolamd2:amd64 (1:5.1.2-2) ...\n",
            "Selecting previously unselected package libcolamd2:amd64.\n",
            "Preparing to unpack .../07-libcolamd2_1%3a5.1.2-2_amd64.deb ...\n",
            "Unpacking libcolamd2:amd64 (1:5.1.2-2) ...\n",
            "Selecting previously unselected package libmetis5:amd64.\n",
            "Preparing to unpack .../08-libmetis5_5.1.0.dfsg-5_amd64.deb ...\n",
            "Unpacking libmetis5:amd64 (5.1.0.dfsg-5) ...\n",
            "Selecting previously unselected package libcholmod3:amd64.\n",
            "Preparing to unpack .../09-libcholmod3_1%3a5.1.2-2_amd64.deb ...\n",
            "Unpacking libcholmod3:amd64 (1:5.1.2-2) ...\n",
            "Selecting previously unselected package libcxsparse3:amd64.\n",
            "Preparing to unpack .../10-libcxsparse3_1%3a5.1.2-2_amd64.deb ...\n",
            "Unpacking libcxsparse3:amd64 (1:5.1.2-2) ...\n",
            "Selecting previously unselected package libglpk40:amd64.\n",
            "Preparing to unpack .../11-libglpk40_4.65-1_amd64.deb ...\n",
            "Unpacking libglpk40:amd64 (4.65-1) ...\n",
            "Selecting previously unselected package libgmpxx4ldbl:amd64.\n",
            "Preparing to unpack .../12-libgmpxx4ldbl_2%3a6.1.2+dfsg-2_amd64.deb ...\n",
            "Unpacking libgmpxx4ldbl:amd64 (2:6.1.2+dfsg-2) ...\n",
            "Selecting previously unselected package libgmp-dev:amd64.\n",
            "Preparing to unpack .../13-libgmp-dev_2%3a6.1.2+dfsg-2_amd64.deb ...\n",
            "Unpacking libgmp-dev:amd64 (2:6.1.2+dfsg-2) ...\n",
            "Selecting previously unselected package libgraphblas1:amd64.\n",
            "Preparing to unpack .../14-libgraphblas1_1%3a5.1.2-2_amd64.deb ...\n",
            "Unpacking libgraphblas1:amd64 (1:5.1.2-2) ...\n",
            "Selecting previously unselected package libklu1:amd64.\n",
            "Preparing to unpack .../15-libklu1_1%3a5.1.2-2_amd64.deb ...\n",
            "Unpacking libklu1:amd64 (1:5.1.2-2) ...\n",
            "Selecting previously unselected package libldl2:amd64.\n",
            "Preparing to unpack .../16-libldl2_1%3a5.1.2-2_amd64.deb ...\n",
            "Unpacking libldl2:amd64 (1:5.1.2-2) ...\n",
            "Selecting previously unselected package libumfpack5:amd64.\n",
            "Preparing to unpack .../17-libumfpack5_1%3a5.1.2-2_amd64.deb ...\n",
            "Unpacking libumfpack5:amd64 (1:5.1.2-2) ...\n",
            "Selecting previously unselected package librbio2:amd64.\n",
            "Preparing to unpack .../18-librbio2_1%3a5.1.2-2_amd64.deb ...\n",
            "Unpacking librbio2:amd64 (1:5.1.2-2) ...\n",
            "Selecting previously unselected package libspqr2:amd64.\n",
            "Preparing to unpack .../19-libspqr2_1%3a5.1.2-2_amd64.deb ...\n",
            "Unpacking libspqr2:amd64 (1:5.1.2-2) ...\n",
            "Selecting previously unselected package libsuitesparse-dev:amd64.\n",
            "Preparing to unpack .../20-libsuitesparse-dev_1%3a5.1.2-2_amd64.deb ...\n",
            "Unpacking libsuitesparse-dev:amd64 (1:5.1.2-2) ...\n",
            "Selecting previously unselected package libglpk-dev:amd64.\n",
            "Preparing to unpack .../21-libglpk-dev_4.65-1_amd64.deb ...\n",
            "Unpacking libglpk-dev:amd64 (4.65-1) ...\n",
            "Selecting previously unselected package libgmp3-dev.\n",
            "Preparing to unpack .../22-libgmp3-dev_2%3a6.1.2+dfsg-2_amd64.deb ...\n",
            "Unpacking libgmp3-dev (2:6.1.2+dfsg-2) ...\n",
            "Selecting previously unselected package libpython3.8-stdlib:amd64.\n",
            "Preparing to unpack .../23-libpython3.8-stdlib_3.8.0-3~18.04_amd64.deb ...\n",
            "Unpacking libpython3.8-stdlib:amd64 (3.8.0-3~18.04) ...\n",
            "Selecting previously unselected package libpython3.8:amd64.\n",
            "Preparing to unpack .../24-libpython3.8_3.8.0-3~18.04_amd64.deb ...\n",
            "Unpacking libpython3.8:amd64 (3.8.0-3~18.04) ...\n",
            "Selecting previously unselected package libpython3.8-dev:amd64.\n",
            "Preparing to unpack .../25-libpython3.8-dev_3.8.0-3~18.04_amd64.deb ...\n",
            "Unpacking libpython3.8-dev:amd64 (3.8.0-3~18.04) ...\n",
            "Selecting previously unselected package python3.8.\n",
            "Preparing to unpack .../26-python3.8_3.8.0-3~18.04_amd64.deb ...\n",
            "Unpacking python3.8 (3.8.0-3~18.04) ...\n",
            "Selecting previously unselected package python3.8-dev.\n",
            "Preparing to unpack .../27-python3.8-dev_3.8.0-3~18.04_amd64.deb ...\n",
            "Unpacking python3.8-dev (3.8.0-3~18.04) ...\n",
            "Setting up libbtf1:amd64 (1:5.1.2-2) ...\n",
            "Setting up libldl2:amd64 (1:5.1.2-2) ...\n",
            "Setting up libcxsparse3:amd64 (1:5.1.2-2) ...\n",
            "Setting up libpython3.8-minimal:amd64 (3.8.0-3~18.04) ...\n",
            "Setting up libgraphblas1:amd64 (1:5.1.2-2) ...\n",
            "Setting up python3.8-minimal (3.8.0-3~18.04) ...\n",
            "Setting up libsuitesparseconfig5:amd64 (1:5.1.2-2) ...\n",
            "Setting up libmetis5:amd64 (5.1.0.dfsg-5) ...\n",
            "Setting up libcolamd2:amd64 (1:5.1.2-2) ...\n",
            "Setting up libccolamd2:amd64 (1:5.1.2-2) ...\n",
            "Setting up libgmpxx4ldbl:amd64 (2:6.1.2+dfsg-2) ...\n",
            "Setting up libpython3.8-stdlib:amd64 (3.8.0-3~18.04) ...\n",
            "Setting up libpython3.8:amd64 (3.8.0-3~18.04) ...\n",
            "Setting up libgmp-dev:amd64 (2:6.1.2+dfsg-2) ...\n",
            "Setting up libpython3.8-dev:amd64 (3.8.0-3~18.04) ...\n",
            "Setting up python3.8 (3.8.0-3~18.04) ...\n",
            "Setting up librbio2:amd64 (1:5.1.2-2) ...\n",
            "Setting up libcamd2:amd64 (1:5.1.2-2) ...\n",
            "Setting up python3.8-dev (3.8.0-3~18.04) ...\n",
            "Setting up libamd2:amd64 (1:5.1.2-2) ...\n",
            "Setting up libgmp3-dev (2:6.1.2+dfsg-2) ...\n",
            "Setting up libglpk40:amd64 (4.65-1) ...\n",
            "Setting up libklu1:amd64 (1:5.1.2-2) ...\n",
            "Setting up libcholmod3:amd64 (1:5.1.2-2) ...\n",
            "Setting up libspqr2:amd64 (1:5.1.2-2) ...\n",
            "Setting up libumfpack5:amd64 (1:5.1.2-2) ...\n",
            "Setting up libsuitesparse-dev:amd64 (1:5.1.2-2) ...\n",
            "Setting up libglpk-dev:amd64 (4.65-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Collecting glpk\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/26/198ec4b9d1b752404a7ecb104bd1b4bfba711feaadabc0b1407de87adb26/glpk-0.4.6.tar.gz (153kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 15.4MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: glpk\n",
            "  Building wheel for glpk (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for glpk: filename=glpk-0.4.6-cp36-cp36m-linux_x86_64.whl size=155739 sha256=46914671de37702a268dc21c7a9f36cf36a493e20ac982188bb3b6ea1bfc2528\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/73/a2/f5c7a16071d060af049f845d3a470191111c9732dae75c51b5\n",
            "Successfully built glpk\n",
            "Installing collected packages: glpk\n",
            "Successfully installed glpk-0.4.6\n",
            "Collecting rouge-score\n",
            "  Downloading https://files.pythonhosted.org/packages/1f/56/a81022436c08b9405a5247b71635394d44fe7e1dbedc4b28c740e09c2840/rouge_score-0.0.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from rouge-score) (1.19.4)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from rouge-score) (0.10.0)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from rouge-score) (1.15.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from rouge-score) (3.2.5)\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.0.4\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYmVMZJnlqjH"
      },
      "source": [
        "# ILP Summary model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtK7uMXVlpkE"
      },
      "source": [
        "import re\r\n",
        "import os\r\n",
        "import codecs\r\n",
        "from collections import Counter\r\n",
        "\r\n",
        "class State:\r\n",
        "    \"\"\" State class\r\n",
        "    Internal class used as a structure to keep track of the search state in \r\n",
        "    the tabu_search method.\r\n",
        "    Args:\r\n",
        "        subset (set): a subset of sentences\r\n",
        "        concepts (Counter): a set of concepts for the subset\r\n",
        "        length (int): the length in words\r\n",
        "        score (int): the score for the subset\r\n",
        "    \"\"\"\r\n",
        "    def __init__(self):\r\n",
        "        self.subset = set()\r\n",
        "        self.concepts = Counter()\r\n",
        "        self.length = 0\r\n",
        "        self.score = 0\r\n",
        "\r\n",
        "class Sentence:\r\n",
        "    \"\"\"The sentence data structure.\r\n",
        "    Args: \r\n",
        "        tokens (list of str): the list of word tokens.\r\n",
        "        doc_id (str): the identifier of the document from which the sentence\r\n",
        "          comes from.\r\n",
        "        position (int): the position of the sentence in the source document.\r\n",
        "    \"\"\"\r\n",
        "    def __init__(self, tokens, doc_id, position):\r\n",
        "\r\n",
        "        self.tokens = tokens\r\n",
        "        \"\"\" tokens as a list. \"\"\"\r\n",
        "\r\n",
        "        self.doc_id = doc_id\r\n",
        "        \"\"\" document identifier of the sentence. \"\"\"\r\n",
        "\r\n",
        "        self.position = position\r\n",
        "        \"\"\" position of the sentence within the document. \"\"\"\r\n",
        "\r\n",
        "        self.concepts = []\r\n",
        "        \"\"\" concepts of the sentence. \"\"\"\r\n",
        "\r\n",
        "        self.untokenized_form = ''\r\n",
        "        \"\"\" untokenized form of the sentence. \"\"\"\r\n",
        "\r\n",
        "        self.length = 0\r\n",
        "        \"\"\" length of the untokenized sentence. \"\"\"\r\n",
        "\r\n",
        "class LoadFile(object):\r\n",
        "    \"\"\"Objects which inherit from this class have read file functions.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, input_directory):\r\n",
        "        \"\"\"\r\n",
        "        Args:\r\n",
        "            input_file (str): the path of the input file.\r\n",
        "            use_stems (bool): whether stems should be used instead of words,\r\n",
        "              defaults to False.\r\n",
        "        \"\"\"\r\n",
        "        self.input_directory = input_directory\r\n",
        "        self.sentences = []\r\n",
        "\r\n",
        "    def read_documents(self, file_extension=\"txt\"):\r\n",
        "        \"\"\"Read the input files in the given directory.\r\n",
        "        Load the input files and populate the sentence list. Input files are\r\n",
        "        expected to be in one tokenized sentence per line format.\r\n",
        "        Args:\r\n",
        "            file_extension (str): the file extension for input documents,\r\n",
        "              defaults to txt.\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "\r\n",
        "        with codecs.open(self.input_directory,\r\n",
        "                            'r',\r\n",
        "                            'utf-8',errors='replace') as f:\r\n",
        "\r\n",
        "            # load the sentences\r\n",
        "            lines = f.readlines()\r\n",
        "\r\n",
        "            # loop over sentences\r\n",
        "            for i in range(len(lines)):\r\n",
        "\r\n",
        "                # split the sentence into tokens\r\n",
        "                tokens = lines[i].strip().split(' ')\r\n",
        "\r\n",
        "                # add the sentence\r\n",
        "                infile = self.input_directory.split('/')[-1]\r\n",
        "                if len(tokens) > 0:\r\n",
        "                    sentence = Sentence(tokens, infile, i)\r\n",
        "                    untokenized_form = untokenize(tokens)\r\n",
        "                    sentence.untokenized_form = untokenized_form\r\n",
        "                    sentence.length = len(untokenized_form.split(' '))\r\n",
        "                    self.sentences.append(sentence)\r\n",
        "\r\n",
        "def untokenize(tokens):\r\n",
        "    \"\"\"Untokenizing a list of tokens. \r\n",
        "    Args:\r\n",
        "        tokens (list of str): the list of tokens to untokenize.\r\n",
        "    Returns:\r\n",
        "        a string\r\n",
        "    \"\"\"\r\n",
        "    text = u' '.join(tokens)\r\n",
        "    text = re.sub(u\"\\s+\", u\" \", text.strip())\r\n",
        "    text = re.sub(u\" ('[a-z]) \", u\"\\g<1> \", text)\r\n",
        "    text = re.sub(u\" ([\\.;,-]) \", u\"\\g<1> \", text)\r\n",
        "    text = re.sub(u\" ([\\.;,-?!])$\", u\"\\g<1>\", text)\r\n",
        "    text = re.sub(u\" _ (.+) _ \", u\" _\\g<1>_ \", text)\r\n",
        "    text = re.sub(u\" \\$ ([\\d\\.]+) \", u\" $\\g<1> \", text)\r\n",
        "    text = text.replace(u\" ' \", u\"' \")\r\n",
        "    text = re.sub(u\"([\\W\\s])\\( \", u\"\\g<1>(\", text)\r\n",
        "    text = re.sub(u\" \\)([\\W\\s])\", u\")\\g<1>\", text)\r\n",
        "    text = text.replace(u\"`` \", u\"``\")\r\n",
        "    text = text.replace(u\" ''\", u\"''\")\r\n",
        "    text = text.replace(u\" n't\", u\"n't\")\r\n",
        "    text = re.sub(u'(^| )\" ([^\"]+) \"( |$)', u'\\g<1>\"\\g<2>\"\\g<3>', text)\r\n",
        "\r\n",
        "    # times\r\n",
        "    text = re.sub('(\\d+) : (\\d+ [ap]\\.m\\.)', '\\g<1>:\\g<2>', text)\r\n",
        "\r\n",
        "    text = re.sub('^\" ', '\"', text)\r\n",
        "    text = re.sub(' \"$', '\"', text)\r\n",
        "    text = re.sub(u\"\\s+\", u\" \", text.strip())\r\n",
        "\r\n",
        "    return text\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\"\"\" Concept-based ILP summarization methods.\r\n",
        "    authors: Florian Boudin (florian.boudin@univ-nantes.fr)\r\n",
        "             Hugo Mougard (hugo.mougard@univ-nantes.fr)\r\n",
        "    version: 0.2\r\n",
        "    date: May 2015\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# from sume.base import Sentence, State, untokenize, LoadFile\r\n",
        "\r\n",
        "from collections import defaultdict, deque\r\n",
        "\r\n",
        "import os\r\n",
        "import re\r\n",
        "import codecs\r\n",
        "import random\r\n",
        "import sys\r\n",
        "\r\n",
        "import nltk\r\n",
        "import pulp\r\n",
        "\r\n",
        "\r\n",
        "class ConceptBasedILPSummarizer(LoadFile):\r\n",
        "    \"\"\"Implementation of the concept-based ILP model for summarization.\r\n",
        "    The original algorithm was published and described in:\r\n",
        "      * Dan Gillick and Benoit Favre, A Scalable Global Model for Summarization,\r\n",
        "        *Proceedings of the NAACL HLT Workshop on Integer Linear Programming for\r\n",
        "        Natural Language Processing*, pages 10–18, 2009.\r\n",
        "        \r\n",
        "    \"\"\"\r\n",
        "    def __init__(self, input_directory):\r\n",
        "        \"\"\"\r\n",
        "        Args:\r\n",
        "            input_directory (str): the directory from which text documents to\r\n",
        "              be summarized are loaded.\r\n",
        "        \"\"\"\r\n",
        "        self.input_directory = input_directory\r\n",
        "        self.sentences = []\r\n",
        "        self.weights = {}\r\n",
        "        self.c2s = defaultdict(set)\r\n",
        "        self.concept_sets = defaultdict(frozenset)\r\n",
        "        self.stoplist = nltk.corpus.stopwords.words('english')\r\n",
        "        self.stemmer = nltk.stem.snowball.SnowballStemmer('english')\r\n",
        "        self.word_frequencies = defaultdict(int)\r\n",
        "        self.w2s = defaultdict(set)\r\n",
        "\r\n",
        "    def extract_ngrams(self, n=2):\r\n",
        "        \"\"\"Extract the ngrams of words from the input sentences.\r\n",
        "        Args:\r\n",
        "            n (int): the number of words for ngrams, defaults to 2\r\n",
        "        \"\"\"\r\n",
        "        for i, sentence in enumerate(self.sentences):\r\n",
        "\r\n",
        "            # for each ngram of words\r\n",
        "            for j in range(len(sentence.tokens)-(n-1)):\r\n",
        "\r\n",
        "                # initialize ngram container\r\n",
        "                ngram = []\r\n",
        "\r\n",
        "                # for each token of the ngram\r\n",
        "                for k in range(j, j+n):\r\n",
        "                    ngram.append(sentence.tokens[k].lower())\r\n",
        "\r\n",
        "                # do not consider ngrams containing punctuation marks\r\n",
        "                marks = [t for t in ngram if not re.search('[a-zA-Z0-9]', t)]\r\n",
        "                if len(marks) > 0:\r\n",
        "                    continue\r\n",
        "\r\n",
        "                # do not consider ngrams composed of only stopwords\r\n",
        "                stops = [t for t in ngram if t in self.stoplist]\r\n",
        "                if len(stops) == len(ngram):\r\n",
        "                    continue\r\n",
        "\r\n",
        "                # stem the ngram\r\n",
        "                ngram = [self.stemmer.stem(t) for t in ngram]\r\n",
        "\r\n",
        "                # add the ngram to the concepts\r\n",
        "                self.sentences[i].concepts.append(' '.join(ngram))\r\n",
        "\r\n",
        "    def compute_document_frequency(self):\r\n",
        "        \"\"\"Compute the document frequency of each concept.\r\n",
        "        \"\"\"\r\n",
        "        for i in range(len(self.sentences)):\r\n",
        "\r\n",
        "            # for each concept\r\n",
        "            for concept in self.sentences[i].concepts:\r\n",
        "\r\n",
        "                # add the document id to the concept weight container\r\n",
        "                if concept not in self.weights:\r\n",
        "                    self.weights[concept] = set([])\r\n",
        "                self.weights[concept].add(self.sentences[i].doc_id)\r\n",
        "\r\n",
        "        # loop over the concepts and compute the document frequency\r\n",
        "        for concept in self.weights:\r\n",
        "            self.weights[concept] = len(self.weights[concept])\r\n",
        "\r\n",
        "    def compute_word_frequency(self):\r\n",
        "        \"\"\"Compute the frequency of each word in the set of documents. \"\"\"\r\n",
        "\r\n",
        "        for i, sentence in enumerate(self.sentences):\r\n",
        "            for token in sentence.tokens:\r\n",
        "                t = token.lower() \r\n",
        "                if not re.search('[a-zA-Z0-9]', t) or t in self.stoplist:\r\n",
        "                    continue\r\n",
        "                t = self.stemmer.stem(t)\r\n",
        "                self.w2s[t].add(i)\r\n",
        "                self.word_frequencies[t] += 1\r\n",
        "\r\n",
        "    def prune_sentences(self,\r\n",
        "                        mininum_sentence_length=5,\r\n",
        "                        remove_citations=True,\r\n",
        "                        remove_redundancy=True):\r\n",
        "        \"\"\"Prune the sentences.\r\n",
        "        Remove the sentences that are shorter than a given length, redundant\r\n",
        "        sentences and citations from entering the summary.\r\n",
        "        Args:\r\n",
        "            mininum_sentence_length (int): the minimum number of words for a\r\n",
        "              sentence to enter the summary, defaults to 5\r\n",
        "            remove_citations (bool): indicates that citations are pruned,\r\n",
        "              defaults to True\r\n",
        "            remove_redundancy (bool): indicates that redundant sentences are\r\n",
        "              pruned, defaults to True\r\n",
        "        \"\"\"\r\n",
        "        pruned_sentences = []\r\n",
        "\r\n",
        "        # loop over the sentences\r\n",
        "        for sentence in self.sentences:\r\n",
        "\r\n",
        "            # prune short sentences\r\n",
        "            if sentence.length < mininum_sentence_length:\r\n",
        "                continue\r\n",
        "\r\n",
        "            # prune citations\r\n",
        "            first_token, last_token = sentence.tokens[0], sentence.tokens[-1]\r\n",
        "            if remove_citations and \\\r\n",
        "               (first_token == u\"``\" or first_token == u'\"') and \\\r\n",
        "               (last_token == u\"''\" or first_token == u'\"'):\r\n",
        "                continue\r\n",
        "\r\n",
        "            # prune ___ said citations\r\n",
        "            # if remove_citations and \\\r\n",
        "            #     (sentence.tokens[0]==u\"``\" or sentence.tokens[0]==u'\"') and \\\r\n",
        "            #     re.search('(?i)(''|\") \\w{,30} (said|reported|told)\\.$',\r\n",
        "            #               sentence.untokenized_form):\r\n",
        "            #     continue\r\n",
        "\r\n",
        "            # prune identical and almost identical sentences\r\n",
        "            if remove_redundancy:\r\n",
        "                is_redundant = False\r\n",
        "                for prev_sentence in pruned_sentences:\r\n",
        "                    if sentence.tokens == prev_sentence.tokens:\r\n",
        "                        is_redundant = True\r\n",
        "                        break\r\n",
        "\r\n",
        "                if is_redundant:\r\n",
        "                    continue\r\n",
        "\r\n",
        "            # otherwise add the sentence to the pruned sentence container\r\n",
        "            pruned_sentences.append(sentence)\r\n",
        "\r\n",
        "        self.sentences = pruned_sentences\r\n",
        "\r\n",
        "    def prune_concepts(self, method=\"threshold\", value=3):\r\n",
        "        \"\"\"Prune the concepts for efficient summarization.\r\n",
        "        Args:\r\n",
        "            method (str): the method for pruning concepts that can be whether\r\n",
        "              by using a minimal value for concept scores (threshold) or using\r\n",
        "              the top-N highest scoring concepts (top-n), defaults to\r\n",
        "              threshold.\r\n",
        "            value (int): the value used for pruning concepts, defaults to 3.\r\n",
        "        \"\"\"\r\n",
        "        # 'threshold' pruning method\r\n",
        "        if method == \"threshold\":\r\n",
        "\r\n",
        "            # iterates over the concept weights\r\n",
        "            concepts = self.weights.keys()\r\n",
        "            for concept in concepts:\r\n",
        "                if self.weights[concept] < value:\r\n",
        "                    del self.weights[concept]\r\n",
        "\r\n",
        "        # 'top-n' pruning method\r\n",
        "        elif method == \"top-n\":\r\n",
        "\r\n",
        "            # sort concepts by scores\r\n",
        "            sorted_concepts = sorted(self.weights,\r\n",
        "                                     key=lambda x: self.weights[x],\r\n",
        "                                     reverse=True)\r\n",
        "\r\n",
        "            # iterates over the concept weights\r\n",
        "            concepts = self.weights.keys()\r\n",
        "            for concept in concepts:\r\n",
        "                if concept not in sorted_concepts[:value]:\r\n",
        "                    del self.weights[concept]\r\n",
        "\r\n",
        "        # iterates over the sentences\r\n",
        "        for i in range(len(self.sentences)):\r\n",
        "\r\n",
        "            # current sentence concepts\r\n",
        "            concepts = self.sentences[i].concepts\r\n",
        "\r\n",
        "            # prune concepts\r\n",
        "            self.sentences[i].concepts = [c for c in concepts\r\n",
        "                                          if c in self.weights]\r\n",
        "\r\n",
        "    def compute_c2s(self):\r\n",
        "        \"\"\"Compute the inverted concept to sentences dictionary. \"\"\"\r\n",
        "\r\n",
        "        for i, sentence in enumerate(self.sentences):\r\n",
        "            for concept in sentence.concepts:\r\n",
        "                self.c2s[concept].add(i)\r\n",
        "\r\n",
        "    def compute_concept_sets(self):\r\n",
        "        \"\"\"Compute the concept sets for each sentence.\"\"\"\r\n",
        "\r\n",
        "        for i, sentence in enumerate(self.sentences):\r\n",
        "            for concept in sentence.concepts:\r\n",
        "                self.concept_sets[i] |= {concept}\r\n",
        "\r\n",
        "    def greedy_approximation(self, summary_size=100):\r\n",
        "        \"\"\"Greedy approximation of the ILP model.\r\n",
        "        Args:\r\n",
        "            summary_size (int): the maximum size in words of the summary,\r\n",
        "              defaults to 100.\r\n",
        "        Returns:\r\n",
        "            (value, set) tuple (int, list): the value of the approximated\r\n",
        "              objective function and the set of selected sentences as a tuple.\r\n",
        "        \"\"\"\r\n",
        "        # initialize the inverted c2s dictionary if not already created\r\n",
        "        if not self.c2s:\r\n",
        "            self.compute_c2s()\r\n",
        "\r\n",
        "        # initialize weights\r\n",
        "        weights = {}\r\n",
        "\r\n",
        "        # initialize the score of the best singleton\r\n",
        "        best_singleton_score = 0\r\n",
        "\r\n",
        "        # compute indices of our sentences\r\n",
        "        sentences = range(len(self.sentences))\r\n",
        "\r\n",
        "        # compute initial weights and fill the reverse index\r\n",
        "        # while keeping track of the best singleton solution\r\n",
        "        for i, sentence in enumerate(self.sentences):\r\n",
        "            weights[i] = sum(self.weights[c] for c in set(sentence.concepts))\r\n",
        "            if sentence.length <= summary_size\\\r\n",
        "               and weights[i] > best_singleton_score:\r\n",
        "                best_singleton_score = weights[i]\r\n",
        "                best_singleton = i\r\n",
        "\r\n",
        "        # initialize the selected solution properties\r\n",
        "        sel_subset, sel_concepts, sel_length, sel_score = set(), set(), 0, 0\r\n",
        "\r\n",
        "        # greedily select a sentence\r\n",
        "        while True:\r\n",
        "\r\n",
        "            ###################################################################\r\n",
        "            # RETRIEVE THE BEST SENTENCE\r\n",
        "            ###################################################################\r\n",
        "\r\n",
        "            # sort the sentences by gain and reverse length\r\n",
        "            sort_sent = sorted(((weights[i] / float(self.sentences[i].length),\r\n",
        "                                 -self.sentences[i].length,\r\n",
        "                                 i)\r\n",
        "                                for i in sentences),\r\n",
        "                               reverse=True)\r\n",
        "\r\n",
        "            # select the first sentence that fits in the length limit\r\n",
        "            for sentence_gain, rev_length, sentence_index in sort_sent:\r\n",
        "                if sel_length - rev_length <= summary_size:\r\n",
        "                    break\r\n",
        "            # if we don't find a sentence, break out of the main while loop\r\n",
        "            else:\r\n",
        "                break\r\n",
        "\r\n",
        "            # if the gain is null, break out of the main while loop\r\n",
        "            if not weights[sentence_index]:\r\n",
        "                break\r\n",
        "\r\n",
        "            # update the selected subset properties\r\n",
        "            sel_subset.add(sentence_index)\r\n",
        "            sel_score += weights[sentence_index]\r\n",
        "            sel_length -= rev_length\r\n",
        "\r\n",
        "            # update sentence weights with the reverse index\r\n",
        "            for concept in set(self.sentences[sentence_index].concepts):\r\n",
        "                if concept not in sel_concepts:\r\n",
        "                    for sentence in self.c2s[concept]:\r\n",
        "                        weights[sentence] -= self.weights[concept]\r\n",
        "\r\n",
        "            # update the last selected subset property\r\n",
        "            sel_concepts.update(self.sentences[sentence_index].concepts)\r\n",
        "\r\n",
        "        # check if a singleton has a better score than our greedy solution\r\n",
        "        if best_singleton_score > sel_score:\r\n",
        "            return best_singleton_score, set([best_singleton])\r\n",
        "\r\n",
        "        # returns the (objective function value, solution) tuple\r\n",
        "        return sel_score, sel_subset\r\n",
        "\r\n",
        "    def tabu_search(self,\r\n",
        "                    summary_size=100,\r\n",
        "                    memory_size=10,\r\n",
        "                    iterations=100,\r\n",
        "                    mutation_size=2,\r\n",
        "                    mutation_group=True):\r\n",
        "        \"\"\"Greedy approximation of the ILP model with a tabu search\r\n",
        "          meta-heuristic.\r\n",
        "        Args:\r\n",
        "            summary_size (int): the maximum size in words of the summary,\r\n",
        "              defaults to 100.\r\n",
        "            memory_size (int): the maximum size of the pool of sentences\r\n",
        "              to ban at a given time, defaults at 5.\r\n",
        "            iterations (int): the number of iterations to run, defaults at\r\n",
        "              30.\r\n",
        "            mutation_size (int): number of sentences to unselect and add to\r\n",
        "              the tabu list at each iteration.\r\n",
        "            mutation_group (boolean): flag to consider the mutations as a\r\n",
        "              group: we'll check sentence combinations in the tabu list, not\r\n",
        "              sentences alone.\r\n",
        "        Returns:\r\n",
        "            (value, set) tuple (int, list): the value of the approximated\r\n",
        "              objective function and the set of selected sentences as a tuple.\r\n",
        "        \"\"\"\r\n",
        "        # compute concept to sentences and concept sets for each sentence\r\n",
        "        if not self.c2s:\r\n",
        "            self.compute_c2s()\r\n",
        "        if not self.concept_sets:\r\n",
        "            self.compute_concept_sets()\r\n",
        "\r\n",
        "        # initialize weights\r\n",
        "        weights = {}\r\n",
        "\r\n",
        "        # initialize the score of the best singleton\r\n",
        "        best_singleton_score = 0\r\n",
        "\r\n",
        "        # compute initial weights and fill the reverse index\r\n",
        "        # while keeping track of the best singleton solution\r\n",
        "        for i, sentence in enumerate(self.sentences):\r\n",
        "            weights[i] = sum(self.weights[c] for c in set(sentence.concepts))\r\n",
        "            if sentence.length <= summary_size\\\r\n",
        "               and weights[i] > best_singleton_score:\r\n",
        "                best_singleton_score = weights[i]\r\n",
        "                best_singleton = i\r\n",
        "\r\n",
        "        best_subset, best_score = None, 0\r\n",
        "        state = State()\r\n",
        "        for i in xrange(iterations):\r\n",
        "            queue = deque([], memory_size)\r\n",
        "            # greedily select sentences\r\n",
        "            state = self.select_sentences(summary_size,\r\n",
        "                                          weights,\r\n",
        "                                          state,\r\n",
        "                                          queue,\r\n",
        "                                          mutation_group)\r\n",
        "            if state.score > best_score:\r\n",
        "                best_subset = state.subset.copy()\r\n",
        "                best_score = state.score\r\n",
        "            to_tabu = set(random.sample(state.subset, mutation_size))\r\n",
        "            state = self.unselect_sentences(weights, state, to_tabu)\r\n",
        "            queue.extend(to_tabu)\r\n",
        "\r\n",
        "        # check if a singleton has a better score than our greedy solution\r\n",
        "        if best_singleton_score > best_score:\r\n",
        "            return best_singleton_score, set([best_singleton])\r\n",
        "\r\n",
        "        # returns the (objective function value, solution) tuple\r\n",
        "        return best_score, best_subset\r\n",
        "\r\n",
        "    def select_sentences(self,\r\n",
        "                         summary_size,\r\n",
        "                         weights,\r\n",
        "                         state,\r\n",
        "                         tabu_set,\r\n",
        "                         mutation_group):\r\n",
        "        \"\"\"Greedy sentence selector.\r\n",
        "        Args:\r\n",
        "            summary_size (int): the maximum size in words of the summary,\r\n",
        "              defaults to 100.\r\n",
        "            weights (dictionary): the sentence weights dictionary. This\r\n",
        "              dictionnary is updated during this method call (in-place).\r\n",
        "            state (State): the state of the tabu search from which to start\r\n",
        "              selecting sentences.\r\n",
        "            tabu_set (iterable): set of sentences that are tabu: this\r\n",
        "              selector will not consider them.\r\n",
        "            mutation_group (boolean): flag to consider the mutations as a\r\n",
        "              group: we'll check sentence combinations in the tabu list, not\r\n",
        "              sentences alone.\r\n",
        "        Returns:\r\n",
        "            state (State): the new state of the search. Also note that\r\n",
        "              weights is modified in-place.\r\n",
        "        \"\"\"\r\n",
        "        # greedily select a sentence while respecting the tabu\r\n",
        "        while True:\r\n",
        "\r\n",
        "            ###################################################################\r\n",
        "            # RETRIEVE THE BEST SENTENCE\r\n",
        "            ###################################################################\r\n",
        "\r\n",
        "            # sort the sentences by gain and reverse length\r\n",
        "            sort_sent = sorted(((weights[i] / float(self.sentences[i].length),\r\n",
        "                                 -self.sentences[i].length,\r\n",
        "                                 i)\r\n",
        "                                for i in range(len(self.sentences))\r\n",
        "                                if self.sentences[i].length + state.length <=\r\n",
        "                                summary_size),\r\n",
        "                               reverse=True)\r\n",
        "\r\n",
        "            # select the first sentence that fits in the length limit\r\n",
        "            for sentence_gain, rev_length, sentence_index in sort_sent:\r\n",
        "                if mutation_group:\r\n",
        "                    subset = state.subset | {sentence_index}\r\n",
        "                    for tabu in tabu_set:\r\n",
        "                        if tabu <= subset:\r\n",
        "                            break\r\n",
        "                    else:\r\n",
        "                        break\r\n",
        "                else:\r\n",
        "                    if sentence_index not in tabu_set:\r\n",
        "                        break\r\n",
        "            # if we don't find a sentence, break out of the main while loop\r\n",
        "            else:\r\n",
        "                break\r\n",
        "\r\n",
        "            # if the gain is null, break out of the main while loop\r\n",
        "            if not weights[sentence_index]:\r\n",
        "                break\r\n",
        "\r\n",
        "            # update state\r\n",
        "            state.subset |= {sentence_index}\r\n",
        "            state.concepts.update(self.concept_sets[sentence_index])\r\n",
        "            state.length -= rev_length\r\n",
        "            state.score += weights[sentence_index]\r\n",
        "\r\n",
        "            # update sentence weights with the reverse index\r\n",
        "            for concept in set(self.concept_sets[sentence_index]):\r\n",
        "                if state.concepts[concept] == 1:\r\n",
        "                    for sentence in self.c2s[concept]:\r\n",
        "                        weights[sentence] -= self.weights[concept]\r\n",
        "        return state\r\n",
        "\r\n",
        "    def unselect_sentences(self, weights, state, to_remove):\r\n",
        "        \"\"\"Sentence ``un-selector'' (reverse operation of the\r\n",
        "          select_sentences method).\r\n",
        "        Args:\r\n",
        "            weights (dictionary): the sentence weights dictionary. This\r\n",
        "              dictionnary is updated during this method call (in-place).\r\n",
        "            state (State): the state of the tabu search from which to start\r\n",
        "              un-selecting sentences.\r\n",
        "            to_remove (iterable): set of sentences to unselect.\r\n",
        "        Returns:\r\n",
        "            state (State): the new state of the search. Also note that\r\n",
        "              weights is modified in-place.\r\n",
        "        \"\"\"\r\n",
        "        # remove the sentence indices from the solution subset\r\n",
        "        state.subset -= to_remove\r\n",
        "        for sentence_index in to_remove:\r\n",
        "            # update state\r\n",
        "            state.concepts.subtract(self.concept_sets[sentence_index])\r\n",
        "            state.length -= self.sentences[sentence_index].length\r\n",
        "            # update sentence weights with the reverse index\r\n",
        "            for concept in set(self.concept_sets[sentence_index]):\r\n",
        "                if not state.concepts[concept]:\r\n",
        "                    for sentence in self.c2s[concept]:\r\n",
        "                        weights[sentence] += self.weights[concept]\r\n",
        "            state.score -= weights[sentence_index]\r\n",
        "        return state\r\n",
        "\r\n",
        "    def solve_ilp_problem(self,\r\n",
        "                          summary_size=100,\r\n",
        "                          solver='glpk',\r\n",
        "                          excluded_solutions=[],\r\n",
        "                          unique=False):\r\n",
        "        \"\"\"Solve the ILP formulation of the concept-based model.\r\n",
        "        Args:\r\n",
        "            summary_size (int): the maximum size in words of the summary,\r\n",
        "              defaults to 100.\r\n",
        "            solver (str): the solver used, defaults to glpk.\r\n",
        "            excluded_solutions (list of list): a list of subsets of sentences\r\n",
        "              that are to be excluded, defaults to []\r\n",
        "            unique (bool): modify the model so that it produces only one optimal\r\n",
        "              solution, defaults to False\r\n",
        "        Returns:\r\n",
        "            (value, set) tuple (int, list): the value of the objective function\r\n",
        "              and the set of selected sentences as a tuple.\r\n",
        "        \"\"\"\r\n",
        "        # initialize container shortcuts\r\n",
        "        concepts = self.weights.keys()\r\n",
        "        w = self.weights\r\n",
        "        L = summary_size\r\n",
        "        C = len(concepts)\r\n",
        "        S = len(self.sentences)\r\n",
        "\r\n",
        "        if not self.word_frequencies:\r\n",
        "            self.compute_word_frequency()\r\n",
        "\r\n",
        "        tokens = self.word_frequencies.keys()\r\n",
        "        f = self.word_frequencies\r\n",
        "        T = len(tokens)\r\n",
        "\r\n",
        "        # HACK Sort keys\r\n",
        "        concepts = sorted(self.weights, key=self.weights.get, reverse=True)\r\n",
        "\r\n",
        "        # formulation of the ILP problem\r\n",
        "        prob = pulp.LpProblem(self.input_directory, pulp.LpMaximize)\r\n",
        "\r\n",
        "        # initialize the concepts binary variables\r\n",
        "        c = pulp.LpVariable.dicts(name='c_var',\r\n",
        "                                  indexs=range(C),\r\n",
        "                                  lowBound=0,\r\n",
        "                                  upBound=1,\r\n",
        "                                  cat='Integer')\r\n",
        "\r\n",
        "        # initialize the sentences binary variables\r\n",
        "        s = pulp.LpVariable.dicts(name='s_var',\r\n",
        "                                  indexs=range(S),\r\n",
        "                                  lowBound=0,\r\n",
        "                                  upBound=1,\r\n",
        "                                  cat='Integer')\r\n",
        "\r\n",
        "        # initialize the word binary variables\r\n",
        "        t = pulp.LpVariable.dicts(name='t_var',\r\n",
        "                                  indexs=range(T),\r\n",
        "                                  lowBound=0,\r\n",
        "                                  upBound=1,\r\n",
        "                                  cat='Integer')\r\n",
        "\r\n",
        "        # OBJECTIVE FUNCTION\r\n",
        "        prob += sum(w[concepts[i]] * c[i] for i in range(C))\r\n",
        "               \r\n",
        "        if unique:\r\n",
        "            prob += sum(w[concepts[i]] * c[i] for i in range(C)) + \\\r\n",
        "                    10e-6 * sum(f[tokens[k]] * t[k] for k in range(T))\r\n",
        "\r\n",
        "        # CONSTRAINT FOR SUMMARY SIZE\r\n",
        "        prob += sum(s[j] * self.sentences[j].length for j in range(S)) <= L\r\n",
        "\r\n",
        "        # INTEGRITY CONSTRAINTS\r\n",
        "        for i in range(C):\r\n",
        "            for j in range(S):\r\n",
        "                if concepts[i] in self.sentences[j].concepts:\r\n",
        "                    prob += s[j] <= c[i]\r\n",
        "\r\n",
        "        for i in range(C):\r\n",
        "            prob += sum(s[j] for j in range(S)\r\n",
        "                        if concepts[i] in self.sentences[j].concepts) >= c[i]\r\n",
        "\r\n",
        "        # WORD INTEGRITY CONSTRAINTS\r\n",
        "        if unique:\r\n",
        "            for k in range(T):\r\n",
        "                for j in self.w2s[tokens[k]]:\r\n",
        "                    prob += s[j] <= t[k]\r\n",
        "\r\n",
        "            for k in range(T):\r\n",
        "                prob += sum(s[j] for j in self.w2s[tokens[k]]) >= t[k]\r\n",
        "\r\n",
        "        # CONSTRAINTS FOR FINDING OPTIMAL SOLUTIONS\r\n",
        "        for sentence_set in excluded_solutions:\r\n",
        "            prob += sum([s[j] for j in sentence_set]) <= len(sentence_set)-1\r\n",
        "\r\n",
        "        # prob.writeLP('test.lp')\r\n",
        "\r\n",
        "        # solving the ilp problem\r\n",
        "        if solver == 'gurobi':\r\n",
        "            prob.solve(pulp.GUROBI(msg=0),)\r\n",
        "        elif solver == 'glpk':\r\n",
        "            prob.solve(pulp.PULP_CBC_CMD(msg=0),)\r\n",
        "        elif solver == 'cplex':\r\n",
        "            prob.solve(pulp.CPLEX(msg=0),)\r\n",
        "        else:\r\n",
        "            sys.exit('no solver specified')\r\n",
        "\r\n",
        "        # retreive the optimal subset of sentences\r\n",
        "        solution = set([j for j in range(S) if s[j].varValue == 1])\r\n",
        "\r\n",
        "        # returns the (objective function value, solution) tuple\r\n",
        "        return (pulp.value(prob.objective), solution)\r\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oTClO6E2PrK"
      },
      "source": [
        "## Summary with ILP model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgLYpWzpjNdG"
      },
      "source": [
        "\r\n",
        "def summary_text(dir_path):\r\n",
        "    \"\"\"\r\n",
        "    Summary text document. \r\n",
        "    Args: \r\n",
        "        dir_path: directory to the document. The document is expected to has one sentence each line.\r\n",
        "    \"\"\"\r\n",
        "    s = ConceptBasedILPSummarizer(dir_path)\r\n",
        "    s.read_documents(file_extension=\"txt\")\r\n",
        "    s.extract_ngrams()\r\n",
        "    s.compute_document_frequency()\r\n",
        "    s.prune_sentences(mininum_sentence_length=10,\r\n",
        "                    remove_citations=True,\r\n",
        "                    remove_redundancy=True)\r\n",
        "    value, subset = s.solve_ilp_problem()\r\n",
        "\r\n",
        "    summary = '\\n'.join([s.sentences[j].untokenized_form for j in subset])\r\n",
        "    return summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZBHUZIV2xgR"
      },
      "source": [
        "### Util function help split text to one sentence each line"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhVtRcKn2fri"
      },
      "source": [
        "# sentence_tokenizer  =  nltk.data.load('tokenizers/punkt/english.pickle')\r\n",
        "def split_sentence(data_path):\r\n",
        "    filenames = os.listdir(data_path)\r\n",
        "\r\n",
        "    for filename in filenames:\r\n",
        "        lines = []\r\n",
        "        with open(os.path.join(data_path, filename), 'r',errors = 'replace' ) as f:\r\n",
        "            line = f.read()\r\n",
        "            lines.extend(sentence_tokenizer.tokenize(line.rstrip()))\r\n",
        "     \r\n",
        "        with open(os.path.join(data_path, filename), 'w',errors = 'replace') as f:\r\n",
        "            for line in lines:\r\n",
        "                f.write(line + '\\n')\r\n",
        "    # for filename in filenames:\r\n",
        "    #     print(filename)\r\n",
        "    #     with open(os.path.join(data_path, filename), 'w') as f:\r\n",
        "    #         for line in lines:\r\n",
        "    #             f.write(line + '\\n')\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# path = '/content/drive/MyDrive/nlp projects/text_summarization/BBC News Summary/News Articles/tech'\r\n",
        "# split_sentence(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "id": "YAfip3nS2dcq",
        "outputId": "c3090314-bb31-46b7-b8d5-ef06e9341f9a"
      },
      "source": [
        "summary_text('/content/drive/MyDrive/nlp projects/text_summarization/BBC News Summary/News Articles/business/009.txt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pulp/pulp.py:1199: UserWarning: Spaces are not permitted in the name. Converted to '_'\n",
            "  warnings.warn(\"Spaces are not permitted in the name. Converted to '_'\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'In 2003, crop production totalled 11.49 million tonnes, the joint report from the Food and Agriculture Organisation and the World Food Programme said.\\nGood rains, increased use of fertilizers and improved seeds contributed to the rise in production.\\nIn eastern and southern Ethiopia, a prolonged drought has killed crops and drained wells.\\n\"Local purchase of cereals for food assistance programmes is recommended as far as possible, so as to assist domestic markets and farmers,\" said Henri Josserand, chief of FAO\\'s Global Information and Early Warning System.\\nAgriculture is the main economic activity in Ethiopia, representing 45% of gross domestic product.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjhZuIFJ3BER"
      },
      "source": [
        "### Rouge scoring"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loxXXInZnH3Q"
      },
      "source": [
        "\r\n",
        "scorer = rouge_scorer.RougeScorer([ 'rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\r\n",
        "\r\n",
        "def rouge_scoring(data_path, sum_path):\r\n",
        "    all_summaries = []\r\n",
        "    filenames = os.listdir(data_path)\r\n",
        "    for filename in filenames:\r\n",
        "        all_summaries.append(summary_text(os.path.join(data_path, filename)))\r\n",
        "\r\n",
        "    for i  in range(len(all_summaries)):\r\n",
        "        all_summaries[i] = all_summaries[i].replace('\\n', ' ')\r\n",
        "\r\n",
        "    scores = []\r\n",
        "    \r\n",
        "    for i  in range(len(filenames)):\r\n",
        "        with open(os.path.join(sum_path, filenames[i]), 'r') as f:\r\n",
        "            reference = f.read().rstrip()\r\n",
        "            scores.append(scorer.score(reference, all_summaries[i] ))\r\n",
        "\r\n",
        "\r\n",
        "    all_rouge1 = []\r\n",
        "    all_rouge2 = []\r\n",
        "    all_rougeL = []\r\n",
        "    for dic in scores:\r\n",
        "        all_rouge1.append(dic['rouge1'][2])\r\n",
        "        all_rouge2.append(dic['rouge2'][2])\r\n",
        "        all_rougeL.append(dic['rougeL'][2])\r\n",
        "\r\n",
        "    return (np.mean(all_rouge1), np.mean(all_rouge2), np.mean(all_rougeL))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "    "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyVh0XhAwvkC"
      },
      "source": [
        "# summary and scoring all the document in a dir. \r\n",
        "data_path = '/content/drive/MyDrive/nlp projects/text_summarization/BBC News Summary/News Articles/business'\r\n",
        "sum_path = '/content/drive/MyDrive/nlp projects/text_summarization/BBC News Summary/Summaries/business'\r\n",
        "\r\n",
        "scores = rouge_scoring(data_path, sum_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Ja9o8yh-KhB",
        "outputId": "5f1ed9fe-9075-482f-b992-c589f4da47f4"
      },
      "source": [
        "# Available solver\r\n",
        "pulp.list_solvers(onlyAvailable= True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['PULP_CBC_CMD', 'PULP_CHOCO_CMD']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "or800XnnL9U3"
      },
      "source": [
        "## Sume CNN dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCz44MYQw-56"
      },
      "source": [
        "import pandas as pd\r\n",
        "cnn = pd.read_csv('/content/cnn.csv',error_bad_lines=False, index_col = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99G1aXCMQMB-"
      },
      "source": [
        "for i in range(len(cnn)):\r\n",
        "    cnn.iloc[i, 0] = re.sub(r'b\"', '', cnn.iloc[i, 0])\r\n",
        "    cnn.iloc[i, 1] = re.sub(r'b\"', '', cnn.iloc[i, 1])\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CU-LYT6uNJop"
      },
      "source": [
        "# sentence_tokenizer  =  nltk.data.load('tokenizers/punkt/english.pickle')\r\n",
        "\r\n",
        "# cnn_path = '/content/drive/MyDrive/nlp projects/text_summarization/CNN_2000'\r\n",
        "# for i in range(len(cnn)):\r\n",
        "#     all_lines = sentence_tokenizer.tokenize(cnn.iloc[i, 0])\r\n",
        "#     with open(os.path.join(cnn_path, 'article', str(i) + '.txt'), 'w') as f:\r\n",
        "#         for line in all_lines:\r\n",
        "#             f.write(line + '\\n')\r\n",
        "#     with open(os.path.join(cnn_path, 'summary', str(i) + '.txt'), 'w') as f:\r\n",
        "#         f.write(cnn.iloc[i, 1])\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiAl6OLWQIrl",
        "outputId": "d168eb07-ff1d-4dd4-a7b3-a50c21952d03"
      },
      "source": [
        "data_path = '/content/drive/MyDrive/nlp projects/text_summarization/CNN_2000/article'\r\n",
        "sum_path = '/content/drive/MyDrive/nlp projects/text_summarization/CNN_2000/summary'\r\n",
        "\r\n",
        "scores = rouge_scoring(data_path, sum_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pulp/pulp.py:1199: UserWarning: Spaces are not permitted in the name. Converted to '_'\n",
            "  warnings.warn(\"Spaces are not permitted in the name. Converted to '_'\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpQAXC_4SnDm",
        "outputId": "44997f55-4644-49cd-c1aa-9a570429e1f5"
      },
      "source": [
        "scores"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.2919275486473456, 0.09819238611267934, 0.1755400098591794)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jpzio7dXlQx"
      },
      "source": [
        "# BERT-extractive-summarizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghQhW-rxAWuK",
        "outputId": "ab4ed851-7da5-4d70-d6f5-9987929e5285"
      },
      "source": [
        "!pip install bert-extractive-summarizer"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-extractive-summarizer\n",
            "  Downloading https://files.pythonhosted.org/packages/e4/73/a1faff60824aa5a3746dc6b73ad666b936250178f296a09ddd13911c8c86/bert_extractive_summarizer-0.6.1-py3-none-any.whl\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (from bert-extractive-summarizer) (2.2.4)\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/0c/7d5950fcd80b029be0a8891727ba21e0cd27692c407c51261c3c921f6da3/transformers-4.1.1-py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 16.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from bert-extractive-summarizer) (0.22.2.post1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->bert-extractive-summarizer) (0.8.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->bert-extractive-summarizer) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->bert-extractive-summarizer) (3.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy->bert-extractive-summarizer) (51.1.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy->bert-extractive-summarizer) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy->bert-extractive-summarizer) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy->bert-extractive-summarizer) (1.19.4)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy->bert-extractive-summarizer) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy->bert-extractive-summarizer) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy->bert-extractive-summarizer) (4.41.1)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->bert-extractive-summarizer) (0.4.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->bert-extractive-summarizer) (7.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->bert-extractive-summarizer) (2.0.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers->bert-extractive-summarizer) (3.0.12)\n",
            "Collecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 51.8MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 49.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers->bert-extractive-summarizer) (20.8)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers->bert-extractive-summarizer) (0.8)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers->bert-extractive-summarizer) (2019.12.20)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->bert-extractive-summarizer) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->bert-extractive-summarizer) (1.0.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy->bert-extractive-summarizer) (3.3.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers->bert-extractive-summarizer) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers->bert-extractive-summarizer) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers->bert-extractive-summarizer) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->bert-extractive-summarizer) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->bert-extractive-summarizer) (3.4.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=7cb758e967e8f295b4452049d40f0338ea43d2f2c7bf9e4338962e70eee7a5cd\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers, bert-extractive-summarizer\n",
            "Successfully installed bert-extractive-summarizer-0.6.1 sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163,
          "referenced_widgets": [
            "add62610e9cc4ebeb8948a6e4bfe2811",
            "1d351bb9963546eea1c4a9037a88bf3d",
            "13352c2ea9c040d8a8a1c204fb9e0ef5",
            "e09d4d17f1b44a5a99717522df7da93e",
            "a099ca16de634596b1de973c766835f8",
            "94c300253f324f6b9ac0545503c40bca",
            "6857bedeebde418da06c0de31f802a82",
            "d2c8d793d6594f3e8d2af985a2bfb76f",
            "a7161ba935ac4aebac9b46e0519a8e32",
            "8226e1a83e4d43359acc1b6c0e54b033",
            "d245b87566114d429e8501dd882ce479",
            "55e1dcb4088940fe9f3b59845ef702e5",
            "360bc5ba3fb449dfa63b8ae24060c7a3",
            "8bb06d27851247da919deb208657e731",
            "bb5e553f7f2f433da24f175e60062cc3",
            "f277b1e1f8094e5592e45d0f3e9be0a5",
            "6f80d0e4f1f846299a30418bb580b724",
            "14b56cc9ae6c49e48ac639df1594a4b2",
            "ce96f39439424a1793b6d7b48877f947",
            "38fdfa26c34f496c944f440b66ffa5ee",
            "068d8478fc6a4849a0ee057672786737",
            "209ad5b11a00494ab10c123405ce356c",
            "fd36e56966b44e22ba6c4dd274e2261c",
            "653c2e1c7b6a409080f4327a731d1881"
          ]
        },
        "id": "yzqa_GIeyWat",
        "outputId": "eb2a0121-3432-4684-ae3a-7446bbf61db1"
      },
      "source": [
        "from summarizer import Summarizer\r\n",
        "model = Summarizer()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "add62610e9cc4ebeb8948a6e4bfe2811",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=434.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a7161ba935ac4aebac9b46e0519a8e32",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1344997306.0, style=ProgressStyle(descr…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6f80d0e4f1f846299a30418bb580b724",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "d2RsrAYD3l-j",
        "outputId": "4442071b-b8bd-403c-8721-c9f95e7c59aa"
      },
      "source": [
        "s = open('/content/drive/MyDrive/nlp projects/text_summarization/BBC News Summary/News Articles/business/009.txt', 'r').read()\r\n",
        "model(s, ratio = 0.35)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Ethiopia's crop production up 24%\\n\\nEthiopia produced 14.27 million tonnes of crops in 2004, 24% higher than in 2003 and 21% more than the average of the past five years, a report says. Nevertheless, 2.2 million Ethiopians will still need emergency assistance. Last year, a total of 965,000 tonnes of food assistance was needed to help seven million Ethiopians. The Food and Agriculture Organisation (FAO) recommend that the food assistance is bought locally. Agriculture is the main economic activity in Ethiopia, representing 45% of gross domestic product.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFkk6-pQzlD1"
      },
      "source": [
        "\r\n",
        "\r\n",
        "def rouge_score_bert(df):\r\n",
        "    all_scores = []\r\n",
        "    for i in range(len(df)):\r\n",
        "        candidate = model(df.iloc[i, 0], ratio = 0.5)\r\n",
        "        all_scores.append(scorer.score(df.iloc[i, 1] ,candidate),)\r\n",
        "        \r\n",
        "    return all_scores\r\n",
        "\r\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "uCQ-heOT4QHm",
        "outputId": "9fa53482-dc99-4465-bc73-bab0b1a88a21"
      },
      "source": [
        "import pandas as pd\r\n",
        "df = pd.read_csv('/content/BBC.csv',)\r\n",
        "df.head()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>article</th>\n",
              "      <th>summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Dollar gains on Greenspan speech\\n\\nThe dollar...</td>\n",
              "      <td>The dollar has hit its highest level against t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Ad sales boost Time Warner profit\\nQuarterly p...</td>\n",
              "      <td>TimeWarner said fourth quarter sales rose 2% t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Yukos unit buyer faces loan claim\\n\\nThe owner...</td>\n",
              "      <td>Yukos' owner Menatep Group says it will ask Ro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Pernod takeover talk lifts Domecq\\n\\nShares in...</td>\n",
              "      <td>Pernod has reduced the debt it took on to fund...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>High fuel prices hit BA's profits\\n\\nBritish A...</td>\n",
              "      <td>Rod Eddington, BA's chief executive, said the ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             article                                            summary\n",
              "0  Dollar gains on Greenspan speech\\n\\nThe dollar...  The dollar has hit its highest level against t...\n",
              "1  Ad sales boost Time Warner profit\\nQuarterly p...  TimeWarner said fourth quarter sales rose 2% t...\n",
              "2  Yukos unit buyer faces loan claim\\n\\nThe owner...  Yukos' owner Menatep Group says it will ask Ro...\n",
              "3  Pernod takeover talk lifts Domecq\\n\\nShares in...  Pernod has reduced the debt it took on to fund...\n",
              "4  High fuel prices hit BA's profits\\n\\nBritish A...  Rod Eddington, BA's chief executive, said the ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yX88zkEx6Mbl"
      },
      "source": [
        "import re\r\n",
        "for i in range(len(df)):\r\n",
        "    df.iloc[i, 0] = re.sub(r\"b'\", '', df.iloc[i, 0])\r\n",
        "    df.iloc[i, 0] = re.sub(r'b\"', '', df.iloc[i, 0])\r\n",
        "\r\n",
        "    df.iloc[i, 1] = re.sub(r\"b'\", '', df.iloc[i, 1])\r\n",
        "    df.iloc[i, 1] = re.sub(r'b\"', '', df.iloc[i, 1])\r\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKeEhTGHXAaS"
      },
      "source": [
        "all_scores = rouge_score_bert(df)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6i2LSi-ump0",
        "outputId": "27259eae-7d24-4b5e-e081-bbb0f445c912"
      },
      "source": [
        "all_scores[:5]"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'rouge1': Score(precision=0.6440677966101694, recall=0.6705882352941176, fmeasure=0.6570605187319885),\n",
              "  'rouge2': Score(precision=0.5, recall=0.5207100591715976, fmeasure=0.5101449275362319),\n",
              "  'rougeL': Score(precision=0.4180790960451977, recall=0.43529411764705883, fmeasure=0.42651296829971186)},\n",
              " {'rouge1': Score(precision=0.49783549783549785, recall=0.7516339869281046, fmeasure=0.5989583333333334),\n",
              "  'rouge2': Score(precision=0.3695652173913043, recall=0.5592105263157895, fmeasure=0.4450261780104712),\n",
              "  'rougeL': Score(precision=0.26406926406926406, recall=0.39869281045751637, fmeasure=0.31770833333333337)},\n",
              " {'rouge1': Score(precision=0.6038961038961039, recall=0.7153846153846154, fmeasure=0.6549295774647886),\n",
              "  'rouge2': Score(precision=0.477124183006536, recall=0.5658914728682171, fmeasure=0.5177304964539008),\n",
              "  'rougeL': Score(precision=0.3961038961038961, recall=0.46923076923076923, fmeasure=0.4295774647887323)},\n",
              " {'rouge1': Score(precision=0.5460526315789473, recall=0.7094017094017094, fmeasure=0.6171003717472119),\n",
              "  'rouge2': Score(precision=0.4768211920529801, recall=0.6206896551724138, fmeasure=0.5393258426966292),\n",
              "  'rougeL': Score(precision=0.48026315789473684, recall=0.6239316239316239, fmeasure=0.5427509293680298)},\n",
              " {'rouge1': Score(precision=0.5632183908045977, recall=0.47342995169082125, fmeasure=0.5144356955380578),\n",
              "  'rouge2': Score(precision=0.32947976878612717, recall=0.2766990291262136, fmeasure=0.30079155672823216),\n",
              "  'rougeL': Score(precision=0.3505747126436782, recall=0.2946859903381642, fmeasure=0.3202099737532808)}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYdh6eWmoO6p"
      },
      "source": [
        "all_r1 = []\r\n",
        "all_r2 = []\r\n",
        "all_l = []\r\n",
        "for score in all_scores:\r\n",
        "    all_r1.append(score['rouge1'][2])\r\n",
        "    all_r2.append(score['rouge2'][2])\r\n",
        "    all_l.append(score['rougeL'][2])"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFoQHebko3Iy",
        "outputId": "1598bed1-371d-421d-ff16-241e7ff4f9a9"
      },
      "source": [
        "np.mean(all_l\r\n",
        "        )"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3881354250227339"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpFnqnCoorw0",
        "outputId": "cfaea922-1cf5-4fc7-b9d4-0df70a50657e"
      },
      "source": [
        "all_scores[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'rouge1': Score(precision=0.1559633027522936, recall=0.4857142857142857, fmeasure=0.23611111111111113),\n",
              " 'rouge2': Score(precision=0.046296296296296294, recall=0.14705882352941177, fmeasure=0.0704225352112676),\n",
              " 'rougeL': Score(precision=0.11009174311926606, recall=0.34285714285714286, fmeasure=0.16666666666666669)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iF-xia9XEb5",
        "outputId": "0108879d-2042-4434-df9a-78f920a18750"
      },
      "source": [
        "def bert_bbc(data_path, sum_path):\r\n",
        "    filenames = os.listdir(data_path)\r\n",
        "    for filename in filenames:\r\n",
        "        article = open(os.path.join)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'rouge1': Score(precision=0.21052631578947367, recall=0.34285714285714286, fmeasure=0.2608695652173913),\n",
              " 'rouge2': Score(precision=0.07142857142857142, recall=0.11764705882352941, fmeasure=0.08888888888888889),\n",
              " 'rougeL': Score(precision=0.12280701754385964, recall=0.2, fmeasure=0.15217391304347827)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zj6RUOeuz6cN"
      },
      "source": [
        "def bert_summary_text(file_path):\r\n",
        "    text = ''\r\n",
        "    with open(file_path, 'r') as f:\r\n",
        "        for line in f.readlines():\r\n",
        "            text += line.rstrip()\r\n",
        "    return text\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}